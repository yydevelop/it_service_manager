# コピーテキスト（Markdown変換）

---

## ＥＣサービス（汎用）

私は、衣服などの商品を扱っているA社のITサービス部に所属するITサービスマネージャである。A社は店舗を持たず、東京に本社と物流拠点だけを展開し、ネット販売を行っている。従業員は約300人、登録顧客は約1万人で、我々の部門では、従業員に対するすべてのITサービス（販売管理や財務会計、電子メール、ファイルサーバ等のシステムで提供されている）と、登録顧客に対するネットショッピングサービス（以下、ECサービス）を提供している。

今回私が論述するITサービスは、このうちECサービスである。本サービスはECシステムで提供されており、内部規約で営業部門と次のようなSLAを締結している。

- オンラインサービス提供時間は、24時間365日（計画停止を除く）
- 稼働率99.5%（月間で４時間の停止は許されない）

毎日18時～20時がECサービスの業務ピークであり、トランザクション量が平日平均値の2倍程度まで増加する傾向にある。また、B社営業部では会員の獲得に向けた営業活動やタイムセールなどの販売促進のための企画を行っている。特に、毎週土曜日の18時～20時に実施するタイムセールが好評で、平日平均値の4倍程度のトランザクション量が発生することもある。

ECサービスの利用者は、購入準備のためにECサービスの商品ページを見て回る。このようなウィンドウショッピング的な行動と購入履歴を分析して、新商品の開発につなげるための販売データ分析機能が存在している。

ITサービス部の下にサービスデスクが配置され、そこでは、ITサービスに関するサービス利用部門からの問い合わせに対して、サービスデスク要員がメールや電話で対応している。

---

## 保険契約システムのサービスデスク

中堅保険会社のS社は代理店パートナーと共に、個人および法人向けに保険製品を提供している。S社は保険申し込みの処理や契約管理等の業務に保険契約システムを使用しており、S社の社員および全国の代理店も利用している。今回論述対象とするのは、保険契約システムのサービスデスクであり、S社のIT子会社であるR社がITサービスとして提供している。私は、R社に所属するITサービスマネージャとしてサービスデスク業務に携わった。

保険契約システムの利用者は、システムに問題があったり操作がわからなかったりすると、サービスデスクに問い合わせがを行う。サービスデスクの対応時間は8時から22時までであり、R社のコールセンター内でサービスデスク業務を行っている。２交代で対応するようにチームを構成している。

---

## 自動車販売

自動車販売業を営むA社は、自社社員が使用する営業支援システム、販売管理システム、顧客管理システムなどの社内システム（以下、A社システム）を保有しており業務で使用している。A社は自社リソースの最適化とITサービスマネジメントスキルを有した専門組織によるシステム運用の安定化・最適化の観点化らA社システムの運用を外部会社にアウトソースすることにした。私はA社に対してA社システムの運用作業およびサービスデスクをITサービスとして提供するR社に所属するITサービスマネージャとして、顧客であるA社との～に携わった。

---

## 一般消費者向け商品の製造販売

A社は、一般消費者向けの商品を製造販売している。A社の情報システム部は、販売システムを運用している。販売システムは、社外の一般消費者及びA社営業部の営業部員に、販売サービスとして提供されている。一般消費者はインターネットを使って販売サービスを利用し、A社商品の検索と注文を行う。営業部員は、A社社内のPCを使って販売サービスを利用し、商品の新規登録・削除及び商品の価格変更を行う。

販売サービスのサービスレベル目標は以下の通り。

- サービス時間：24時間365日
- サービス稼働率：99.8%以上
- インシデント発生時のサービス回復時間：４時間以内
- オンライン応答時間：商品検索：平均２秒以内、商品登録：平均５秒以内

---

## 受注管理サービス

P社は、複合機・プリンターなどの印刷用機器及び関連商品をBtoBで販売する中堅企業である。A社のシステム部は受注システムのオンライン処理を受注業務サービスとして提供している。利用者は、A社の営業部の営業担当者である。利用者は、受注業務サービスを利用して商品の受注業務を行う。

受注システムが稼働する業務サーバのハードウェアの保守期限切れに伴い、システム部は受注システムを再構築し、新受注業務サービス（以下、新サービス）を提供することとなった。再構築前の受注システムは、システム障害の発生によって、利用者がサービスを提供することができない状態になることがあり、営業部から新サービスでは同じことが起こらないように注意されていた。新サービスでは、サービス可用性と保守性の向上が求められている点をサービスの特徴として挙げることができる。

---

## 自動車販売管理システム

A社は医療品製造・販売会社であり、関東地区に本社、関西地区に支社、全国に生産工場を展開している。今回論述対象とするのは、製造部が利用する生産管理システムが提供する生産管理サービスであり、A社のIT子会社であるB社がサービスデスクを含めたITサービスとしてA社製造部に対して提供を行っている。加えて、A社は生産管理システムのアプリケーションプログラム管理およびシステム構成機器管理をB社に委託している。

---

## ＥＣサービス（ピーク）

B社は、通信販売事業者であり、衣服などの商品をインターネットで検索して購入することが可能なITサービス（以下、ECサービス）を提供している。

ECサービスはB社が開発・保守を実施するECシステムによって提供されており、B社情報システム部が運用を行っている。ECサービスは計画停止時間帯を除いて、24時間365日サービスの利用が可能である。

毎日18時～20時がECサービスの業務ピークであり、トランザクション量が平日平均値の2倍程度まで増加する傾向にある。また、B社営業部では会員の獲得に向けた営業活動やタイムセールなどの販売促進のための企画を行っている。特に、毎週土曜日の18時～20時に実施するタイムセールが好評で、平日平均値の4倍程度のトランザクション量が発生することもある。

ECサービスの利用者は、購入準備のためにECサービスの商品ページを見て回る。このようなウィンドウショッピング的な行動と購入履歴を分析して、新商品の開発につなげるための販売データ分析機能が存在している。

---

# モジュール

## 変更管理

### 変更管理プロセス

現在の変更プロセスは、ITサービス部門で次の手順で行うルールになっている。

1. RFC（変更要求）の起票と提出
2. RFCの受付・記録・分類
3. CAB（変更諮問委員会）による、RFCの評価
4. 変更の実施

CABは月１回、業務が比較的落ち着いている10日前後の開催を予定している。そこで、それまでに挙がってきたRFCの評価をまとめて行う。そこでRCFの受入が決定したものに対しては、日程を調整してリリースを実施し、問題なければCIの更新を行う。

このような手続きが必要なため、緊急時を除き、RFCの要求から変更の実施まで、通常は1～2か月必要となる。

開発部門とともに、アジャイル開発を採用した場合の変更プロセスを新たに作成することにした。まず、アジャイル開発全体の範囲を決定する段階で、ITサービス部門も参加し、必要に応じて意見を出すとともに、決定事項に関しては、変更箇所を把握しておく。そしてアジャイルのサイクルである２週間ごとに次の①～③を実施することにした。

1. RFC（変更要求）の起票と提出
2. RFCの受付・記録・分類
3. 変更の実施（リリースとCIの更新）

①に関してはアジャイル開発の範囲が決定した段階で運用部門が作成することにした。

②と③の作業は運用部門が実施するが、リリース作業については開発部門に権限を委譲し、責任をもって実施してもらい、CIの更新は運用部門で一元的に実施することにした。このようにアジャイル開発時の変更プロセスを作成することで、管理品質を落とさずに、アジャイル開発による俊敏な対応が可能になると考えた。

---

## ツールによるデプロイの自動化

現在、PGを稼働環境にデプロイする場合は、LBの設定変更によって、受注管理サービスに対する要求振り分け先をSoryyサーバに変更して、すべてのアプリケーションサーバのサービスを停止した後に作業を行っている。この作業の間、1時間程度サービスが利用できない状態となる。私は、利用者からのシステム停止時間をなくしてほしいという要望に答えるために、アプリケーション展開ツール（以下デプロイツール）を導入することとした。デプロイツールの具体的な動作は以下の通りとなる。

- サービスに影響を与えないように、振り分け先のAPサーバをLBで制限し、稼働環境のAPサーバ１台ごとに順次デプロイする。

デプロイツールを使用することで、サービスの停止なくデプロイ作業を完了することができる。合わせて、要員の作業ミスの防止、作業品質の向上も期待できる。

---

## 構成管理

### 磁気テープ装置の故障・CMDB

４台ある磁気テープ装置の一台が故障したので、CMDBを参照して連絡先を調査した。しかし、CMDBに登録されていた保守外車及びメーカーに連絡が取れず、４日間、残りの３台でバックアップを継続した。

私は、サービス停止期間を短縮するためには、古い機器の連絡先の確認が重要だと考えた。そこで、CMDBの中から、取得日から１年以上経過している機器は、システム運用部の担当者に、問い合わせを行い、その顛末をCMDBに登録させることとした。

---

## 運用設計

### SLA

これまで社内で提供するサービスについてはサービスレベル目標を定めていなかったが、新サービスではサービスレベル目標を明確にし、営業部とシステム部との間でSLAを合意することになった。私は非機能要求グレードを参考に主要なサービスレベル項目を洗い出し、営業部とシステム部との間でSLAを設定することとした。私は、営業部の要望を元にした目標値を設定した。この際、営業部とシステム部の間で新サービス開始後１か月の仮のサービスレベル目標とし、正式なサービスレベル目標は１か月後に達成状況を評価後に決定する点を工夫した。これは、現実的に達成可能なSLAを締結するためである。この時、サービスレベル目標の実績値を測定する作業負荷には問題ないことをシステム部内で確認するように留意した。

サービス時間は営業日（月～土曜）の7時～23時までで、第１土曜日の18時～23時までの5時間は計画停止時間として、サービス時間から除く。その中で、サービス稼働率は99.5%を目標とすることとなった。

- システム可用性：月間稼働時間の99.5%以上
- レスポンスタイム：システムの応答時間は平均２秒以下
- 障害対応時間：重大な障害は４時間以内に初期対応、中程度の障害は２４時間以内、軽微な障害は４８時間以内
- サービスデスク：平均初回応答時間１５分以内。解決までの平均時間２時間

---

## インシデント管理

### 障害対応

障害発生時に運用担当者がその場で障害対応内容を迷いながら考えていると、障害対応に時間がかかったり運用担当者が誤った判断をしてしまい２次障害に繋がる恐れがある。そこで私は、運用設計の段階で、障害対応について取るべきアクションを起点に、以下の３つの観点で事前に準備しておくこととした。

- アクション候補：具体的な、障害広報や暫定対策などの行動パターン
- 判断情報：どのアクションを取るか決めるための情報元
- 判断基準：情報をどのような観点で判断するかの基準

アクションを起点に考えたのは、起こる事象を起点にするとパターンが無数に出てしまい、全く同じ事象が発生する可能性が低くなるためである。

### 障害重要度

障害発生時に障害レベルの判定を正しく行い、障害対応の判断に個人によってばらつきを生じないようにするために、私は障害の重大度のレベルと初動対応の方針を設定した。具体的には、レベル４がセキュリティ事故、レベル３が重大事故（サービスの停止）、レベル２が通常障害（サービスの一部停止）、レベル１が部分障害（不便な程度の機能不具合）、レベル０が非障害（軽微な表示上の誤字など）とし、レベル３以上を重大障害に分類し、営業時間外でも対応を行う方針とした。

### インシデントモデル作成

インシデントモデルを作成することとした。インシデントモデルを作成した根拠は、事前に定義した時間内にインシデントを解消できることをA社および営業部に示すことができ、これによってインシデント発生に関わる顧客の不信を抑えれるからである。

### 障害切り分けプロセス

障害対応時間は、障害の検知時間、切り分け時間、復旧作業時間の合計となる。その中では、切り分け時間のばらつきが大きく、時間を要してしまうケースが多かった。私は、切り分けに時間を要する原因は、メンバのスキルにや経験に依存しているためと考えた。運用グループには、入社から日が浅いメンバや、要員ローテーションのために開発グループから移ったばかりのメンバや、運用経験が浅いメンバがおり、障害の切り分けに時間がかかる傾向がみられた。

そこで、品質確保策として、メンバの意見も踏まえて、エスカレーションを含めた障害切り分けプロセスの見直しとマニュアル化を計画立案して実行した。経験のある中堅技術者に参加してもらい、汎用的な障害の調査手順やエスカレーション基準をプロセスフローとしてまとめ、マニュアル化したうえで、タブレットPCを活用して現場で作業しながら作業できることを目指した。

プロセスフローを作成した経験者にシミュレーションを行ってもらったところ、実際の障害発生時にはプロセスにないチェック作業を臨機応変に行っていることがわかった。

私は経験者のノウハウをできるだけ抽出するために、ヒアリングに加えて、ロールプレイングを活用した。具体的には、模擬環境において実機操作を行い、作業項目を引き出しやすくした。この対策によって、ヒアリングだけでは得られなかった多くのプロセスを策定することができた。

---

## 改善（FAQ）

問い合わせからピックアップをして調査した結果、問い合わせの約15%がFAQに掲載されている内容であることがわかった。FAQは利用者が求めている情報を提供するための重要な役割を担っており、FAQの活用度を上げることでサポートデスクへの問い合わせ件数を削減する効果が期待出来た。そこで私は、FAQが問い合わせ者の目に触れやすくし、より多くの利用者が自ら疑問を解決できるよう改善を行った。具体的には、サポートデスクの問い合わせを行う前の画面で一度FAQでの検索を促すことで、問い合わせ者が問い合わせ前にFAQを調べやすくした。また、FAQの構成、掲載方法、回答内容を見直すことでFAQの検索性を高める改善も合わせて実施した。

KPIとしては、問い合わせ件数に占めるFAQに掲載されている問い合わせ件数の割合を10%未満に削減することを設定した。

---

## 改善（アラート対処判断自動化）

夜間帯に発生したアラートは、翌営業日の朝、まとめてベテランの運用担当者がエラーメッセージを確認し、対応要日の判断を手動で行っていた、毎日なんらかのアラートが発生するためミスも起きやすい状態であった。そこで私は、アラート管理ツールを導入してアラートの判断基準の情報を蓄積するようにした。それにより、次回発生した際に対象アラートは自動的に対処要否が判断されるようになった。これにより、月間4000件あったアラートの内、40%が対処不要と割り振られ、その分の対処を減らすことができた。また、アラート管理ツールに対処を行った内容を保存するルールとすることで、俗人化を解消する効果も期待できる。

効果を最大化するために工夫した点として、アラート量と自動処理した割合を可視化することで効果が実感できるようにする点と、入力が滞らないように週１回の定例会議を設けた点である。

---

## アラート発報条件のしきい値見直し

バッチの処理遅延を検知するために遅延監視を入れていたが、データ増加に伴い年々アラートが増加していた。バッチ遅延により業務に影響がある場合は意味があるが、バッチ時間が少し伸びただけという場合も頻発に発報されるため、無駄なアラートになっていた。アラートが発報されるとインシデント管理システムに登録し問題がない旨を記載するための工数もかかっていた。

原因としては、バッチ開発時点で設定されたしきい値が見直されていなかったことだと考え、アラートを一覧化し遅延監視のしきい値の妥当性の見直しを行うことにした。工夫した点として、打合せにユーザ企業のマネージャーも同席してもらい、当該バッチが遅延することによる業務への影響を議論し、適切なしきい値をその場で決めることができるようにした。また、１回限りの改善で終わらないように４半期に一度、アラートの棚卸としきい値の見直しを行う運用とした。

---

## サービスデスク

サービスデスクはサービス利用者からインシデント発生の通知を受付、インシデント管理システムに登録する。

- インシデントに対応の優先度（高・中・低）を割り当てる
- 優先度によって、解決目標時間が定められている
  - （高2時間、中４時間、低８時間）

---

## コミュニケーション

３社同時のビデオミーティング（以下、フォーラム）を開催することを提案して取り決めることにした。フォーラムであれば３社の担当者が同時に顔合わせすることができるので、迅速なコンセンサスを得ることができると考えたためである。

実施後数か月たった後、フォーラムの関連者に対してアンケートを行った結果、フォーラムを行うことで迅速なコンセンサスに寄与しているという解答を得ることができたため、評価としては”良好”であると評価している。
