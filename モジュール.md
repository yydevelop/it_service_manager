# 📖 ITサービスマネージャ論文作成モジュール (ITSM完全対応版)

# プロンプト (SM版)
## 概要
以下のフォーマット制約に基づき、
架空の事例として、「FX」から１つの、合計２つの事例をそれぞれ作成する。

## フォーマット
～と呼ばれる～という特徴を持つ手法を使用し、～をした。なぜならば～という**サービス運用上の問題・課題**が解決できると考えたからである。工夫した点は、～。具体的には～。

## 制約
- 箇条書きや(1)などは使わず、文章形式で
- 300～500文字程度
- 具体的にはのあとは手法を使用した内容の詳細を架空で1例作成する
- 架空の事例特有の**運用・保守・サービス品質**の制約や問題を手法を使用することで解決できるという視点で記述する
- 実施したや達成されたではなく、**行うことにした**レベルに留める
- 定量的な数値を使用する（稼働率、MTTR、インシデント件数、顧客満足度スコアなど）

# 出題テーマ：各管理系の説明
出題テーマである各系の説明は、次のとおりです。

## ① 資源管理系 (Resource Management System)
ハードウェア資源（CPU、ハードディスク、パソコンなど）やソフトウェア資源（プログラム導入手順、構成管理、ライブラリ管理など）の管理を行う。また、構成管理、変更管理、データ資源管理（バックアップ、リストア、世代管理）、ネットワーク資源管理（ネットワーク構成、トラフィックの輻輳）についても扱う。これらの資源の適切性や妥当性を検証し、改善を実施する。

## ② インシデント管理系 (Incident Management System)
サーバー、クライアント、ネットワークなどのシステムにおける障害を監視し、障害が発生した際には応急処置を実施し、原因の調査・特定を行い、記録する。原因が特定された後は、再発防止策を実施する。

## ③ パフォーマンス管理系 (Performance Management System)
システムの容量や資源を最適に配分し、パフォーマンスの向上とコストの削減を図る。目標値、予算、実績値を比較し、差異分析、原因分析、改善策の立案を行う。

## ④ セキュリティ管理系 (Security Management System)
外部からの不正アクセスやウイルスの侵入を防止する。被害やセキュリティインシデントが発生した場合には、迅速な復旧を図る。また、IDやパスワードの厳格な管理に関するユーザーへの啓発・教育活動も行う。

## ⑤ リリース管理系 (Release Management System)
旧システムから新システムへの移行を行う。新システムの導入、事務所移転、大規模メンテナンスなどの場面で実施される。テスト、移行準備、リハーサル、本番切り替えなどのプロセスを含む。

## ⑥ サービスレベル管理系
運用システムのサービス基準を決め、その達成・維持をするための管理項目を設定し、実績把握・差異分析・改善活動を実施します。それを通じて、ユーザの満足度を向上させます。

## ⑦ サービスデスク系
システム利用の相談窓口としての役割を認識し、回答の迅速さ・的確さ・回答範囲の拡大・FAQの体系化・回答データベースの作成・共有などを実施します。

## ⑧ その他全般管理
円滑で効率的な運用を行うために、運用方式・運用標準・性能評価項目・運用体制・障害対応などを設計します。また、運用支援ツールを利用した運用効率の向上・運用要員の育成を含む良好な運用体制・運用環境の変化への対応・新技術の採用等、各種システム運用に関する改善活動を実施します。


# レビュー観点 (SM版)
- **サービスの概要・SLA**
-- 対象となるITサービスの概要と、合意しているサービスレベル（SLA）が明確である
- **出題意図に答える論述**
-- 運用の安定性、信頼性、継続的改善（CSI）の視点で論じている
- **ITサービスマネージャとしての創意工夫、行動力**
-- 予兆検知、根本原因分析、プロセス改善など、SMらしい工夫がある
- **効果の評価と今後の課題**
-- 定量的な品質指標（KPI）を用いた評価と、残存リスクへの認識がある
- **話の趣旨の一貫性**
-- 障害やクレーム（事象）から、対策、予防、改善へと論理がつながっている
- **具体性**
-- 監視項目、運用フロー、体制図などが具体的に説明されている
- **客観性**
-- ログ分析やチケット集計などの事実（ファクト）に基づいて判断している

# ITサービスマネージャ 午後Ⅱ論文用  
## 設問ア汎用文（FX自動取引プラットフォーム）および背景整理

# 1. 設問アの汎用文（FX自動取引プラットフォーム）

私は、FXブローカー向けに自動取引プラットフォームを提供する企業において、  
運用課に所属するITサービスマネージャとして、  
国内複数のFXブローカーとSLAを含むITサービス契約を締結し、  
24時間365日稼働するITサービスの品質管理および運用統制を担当していた。

当該ITサービスは為替市場と連動して稼働しており、  
障害や取引遅延が発生した場合には、  
顧客であるFXブローカーの業務運営や社会的信用に  
直接的な影響を与える特性を持っていた。

そのため、夜間や休日を含めた安定的なサービス提供を実現するため、  
運用課を中心に、開発課および外部委託先と連携しながら、  
インシデント管理、変更管理、サービスレベル管理を通じて、  
SLA達成を前提としたITサービスマネジメントを行っていた。

---


------------------------------------------------------

# FX自動取引プラットフォームの背景（午後Ⅱ論文・複数設問対応用）
## 2. FX自動取引プラットフォームの背景（汎用）

※ 本章は **どの午後Ⅱテーマでも共通利用する汎用背景** とする。

---

## 2.1 事業・サービスの位置づけ（汎用）

- 事業形態  
  - **BtoB型ITサービス事業**
- 提供サービス  
  - **国内FXブローカー向け 自動取引プラットフォーム**
- 顧客  
  - **国内FXブローカー：5社**
- エンドユーザー  
  - FXブローカーと直接契約する個人投資家
- 当社の立場  
  - 投資家とは直接契約せず、**SLAの締結先はFXブローカーのみ**

※  
本サービスは **BtoBtoC構造**であり、  
**ITサービスマネージャの顧客はFXブローカーである**。

---

## 2.2 サービス特性（汎用・定量ブラッシュアップ）

- 為替市場に連動し **24時間365日稼働**
  - 為替市場は月曜早朝（NZ市場）〜土曜早朝（NY市場）まで事実上連続
- 主な処理内容
  - 注文受付、約定処理、ポジション管理
  - 取引履歴・残高データの連携（ブローカー基幹系）
- 性能特性
  - 約定遅延が **数秒単位で損益に直結**
  - ピーク時（重要経済指標発表時）は  
    **平常時の5〜10倍の注文数** が発生

### 障害・性能劣化の影響（因果を明示）

| 影響対象 | 具体的影響 |
|---|---|
| FXブローカー | 約定不可・顧客クレーム増加・監督官庁対応 |
| 投資家 | 機会損失・想定外損失 |
| 当社 | 契約解除・SLA違反・社会的信用低下 |

→ **金融サービスとして「高可用性」「迅速な復旧」「説明責任」が必須**

---

## 2.3 全体の組織構成

経営層
 ├─ CEO（事業責任）
 ├─ CTO（技術責任）
 │
 ├─ 開発課
 │    ├─ アプリケーション開発
 │    └─ クラウドインフラ構築・変更
 │
 └─ 運用課（ITサービス提供部門）
      ├─ ITサービスマネージャ
      ├─ サービスデスク
      └─ 運用担当


- ITサービスマネージャは **運用課に所属**
- 開発課・運用課から独立した「中立な統制・判断」立場

---

## 2.4 チーム構成（運用課・汎用）

- 運用課：**計5名**
  - ITサービスマネージャ：1名
  - サービスデスク：2名
  - 運用担当：2名

※  
- 24時間365日稼働だが **常時有人ではない**
- 内製3交代制は採用せず、**夜間・休日は外部NOC委託**

---

## 2.5 ITサービスマネージャの役割（汎用）

### 主な責任

- SLA / SLO の策定・見直し
- 稼働率・障害状況の分析と経営報告
- 重大インシデント時の統括
  - 優先度判断
  - 顧客説明方針の決定
- 変更管理の最終承認
- エラーバジェットに基づく改善判断

### 実施しない業務（減点回避）

- 障害対応の実作業
- 監視作業
- シフト作成
- 実装・開発

---

## 2.6 サービス提供形態（汎用）

### 監視・一次対応

- 24時間365日：**外部NOC**
- 検知〜通知：**5分以内**

### 障害時フロー

- P1（全社影響・取引不可）
  - 即時オンコール
  - ITサービスマネージャが統括
- P2以下
  - チケット化し営業時間対応

---

# 3. AWS前提の可用性・稼働率設計（定量）

## 3.1 採用クラウドとSLA前提（汎用）

- クラウド基盤：**AWS**
- 主構成
  - EC2（マルチAZ）
  - ALB
  - RDS（Multi-AZ）
- AWS公開SLA（代表値）
  - ALB：**99.99%**
  - EC2：**99.99%**
  - RDS Multi-AZ：**99.95%**

→ **プラットフォーム全体のSLA目標：99.95%**

### 年間停止許容時間

- 99.95%  
  → 年間停止許容 **約4.38時間**
  → 月間 **約22分**

---

## 3.2 稼働率から導かれる「現実的な問題」

### 問題①：AWSは止まらない前提ではない

- AZ障害・リージョン障害は **数年に一度発生**
- 単一リージョン依存では  
  **SLAは達成できても、社会的影響は回避できない**

### 問題②：瞬間的な性能劣化はSLA外

- 約定遅延（1〜3秒）は  
  **SLA違反にならないが、顧客満足度は急落**
- 午後Ⅱでは  
  **「稼働率≠サービス品質」の論点として使える**

---

## 3.3 可用性に対する対策（汎用）

| 観点 | 対策 |
|---|---|
| インフラ | マルチAZ、Auto Scaling |
| 障害検知 | 外形監視＋内部メトリクス |
| 復旧 | 自動復旧＋Runbook |
| 説明責任 | 障害レポート24時間以内提出 |

---

# 4. 午後Ⅱテーマ別「差分パターン」

※ **ここだけを差し替えれば使い回せる**

---

## パターンA：重大インシデント管理

- 想定事象  
  - 経済指標発表時に注文集中 → DB接続枯渇
- 影響  
  - 約定不可：**最大15分**
- 判断
  - 新規注文停止 → クローズのみ許可（部分再開）
- 効果
  - 完全停止を回避、苦情件数 **40%削減**

---

## パターンB：継続性管理（BCP）

- RTO：**30分**
- RPO：**0分（取引データ）**
- 対策
  - DB同期レプリケーション
  - 手動取引切替手順
- 訓練
  - 年2回、机上＋実地

---

## パターンC：移行管理

- 変更対象
  - 約定エンジン刷新
- 停止許容
  - **最大10分**
- 工夫
  - ブルーグリーンデプロイ
  - 切戻し判定を5分で実施

---

## パターンD：外部委託管理（NOC）

- 委託範囲
  - 監視・一次通知
- SLA
  - 検知5分、通知10分
- KPI
  - 通知遅延率：**1%未満**

---

## パターンE：SRE・改善活動

- SLO
  - 約定成功率：**99.99%**
- エラーバジェット
  - 月間 **0.05%**
- 判断
  - 新機能リリース凍結 → 安定化優先


## 5. 午後Ⅱ論文への使い方（重要）

- **2章〜3章：設問アの背景**
- **4章：設問イ・ウで使い分け**
- 数値はすべて  
  - 稼働率
  - 停止時間
  - 件数
  - 割合  
  → 採点者が「現実的」と判断できるレンジ


------------------------------------------------------



## 1) 企業の概要（小〜中堅で採点者がイメージしやすい）

* 業種：BtoBサービス（保守契約＋サポート窓口あり。顧客情報・対応履歴が重要）
* 従業員規模：**230名**

  * これは中小企業基本法の「製造業その他：従業員300人以下」等の枠内で自然な規模感です。([chusho.meti.go.jp][1])
* 拠点：本社1、営業拠点2（計3拠点）
* IT部門（情シス/ITサービス管理の実態に寄せる）

  * 情報システム部門：**正社員8名**
    「従業員1,000名以下では情シス正社員平均が10名以下」という実態調査レンジに合わせています。([インターネットイニシアティブ-IIJ][2])

## 2) ITサービスの概要（対象：顧客管理システム＝CRM）

### 2.1 サービスの位置づけ（令和7っぽく：業務部門×サービスデスク中心）

* ITサービス名：**顧客管理サービス（CRM）**
* 利用部門・利用者数：**90名**

  * 営業：45名、カスタマーサポート：30名、契約・請求（管理）：15名
* 利用時間：**平日 8:45–19:00**（月末は～20:00）
* 取扱情報（個人情報含む）：顧客企業/担当者、契約、問合せ・対応履歴、見積・商談、更新・解約理由

### 2.2 管理対象規模（「2,000社」級の実在導入事例に合わせる）

* 顧客企業（アカウント）数：**約2,000社**
  「既存約2,000社の顧客情報を属人的管理→Salesforceで統合」という導入事例レンジに合わせています。([logical.co.jp][3])
* 顧客担当者情報：**約6,000名**（1社あたり平均3名で管理）
* 契約件数：**約3,000件**（平均1.5契約/社を想定）
* 問合せ（チケット）量：**平均 10件/日（営業日）＝約220件/月＝約2,600件/年**

  * “ITヘルプデスク”の参考値として、実例で「約15チケット/日（営業日）」が示されています。自社はそれより小さめ（230名規模）として10件/日に置いています。([DeNA Engineering][4])

## 3) SLA/SLO（移行・継続性・教育の論点が立つ設定）

### 3.1 可用性（止められないFXと違い「止められるが制約あり」）

* 月間稼働率（SLA）：**99.0%**（業務系として妥当）

  * 99%は「月30日換算で停止許容 7.2時間」という解釈ができ、停止時間制約の説明に使えます。([NTT][5])
* サービス提供時間：**平日 8:45–19:00**
* 性能（SLO）：トップ画面表示 **3秒以内（95パーセンタイル）**（業務ストレスを論点化）

### 3.2 サポートSLA（令和7の“サービスデスク前提”に寄せる）

* 受付：サービスデスク（一次窓口）
* 重大（業務停止/顧客対応停止）

  * 一次応答：**30分以内**
  * 回復目標：**4時間以内**（暫定回復含む）
* 通常（性能劣化/一部機能不具合）

  * 一次応答：**2時間以内**
  * 回復目標：**1営業日以内**

## 4) ITサービスマネージャ（あなたが論述する主体）の具体化

* 所属：情報システム部 **ITサービス管理チーム**
* 役割：**CRMサービスのITサービスマネージャ（1名・意思決定責任者）**
* 責任範囲（午後Ⅱで刺さる“管理”を明記）

  1. サービスレベル管理（SLA/SLO/KPI、レポート、改善計画）
  2. インシデント管理・問題管理の統制（優先度判定、エスカレーション）
  3. 変更/移行管理（リハーサル、切戻し判断、周知）
  4. 継続性管理（RTO/RPO、訓練、代替手段、見直し）
  5. 利用者教育（教育計画、理解度確認、定着化）

## 5) 運用組織・体制（役割分担と人数を固定）

情シス8名の範囲で、令和7っぽい分離（一次受付／運用／開発）を作ります。情シス規模は実態調査の「10名以下」に整合。([インターネットイニシアティブ-IIJ][2])

| 組織/役割       |    人数 | 主な責務                    |
| ----------- | ----: | ----------------------- |
| ITサービスマネージャ |     1 | 方針・判断・合意形成、SLA/BCP/移行統制 |
| サービスデスク（一次） |     2 | 受付、既知対応、FAQ誘導、チケット起票    |
| 運用担当（2次）    |     2 | 監視、権限/マスタ運用、障害切り分け      |
| 開発保守（3次）    |     3 | 改修、データ移行ツール、恒久対策        |
| 合計          | **8** |                         |

### サービスデスクKPI（教育・定着とも連動）

* 一次解決率（FCR）目標：**70〜80%**
  一次解決率の目安として「70〜80%以上を目標にしているケースが多い」が示されています。([services.altius-link.com][6])
* 月間チケット量：**約220件/月**（前述）
* 既知/FAQ誘導比率：**20%→35%**（教育/ナレッジで伸ばす“効果”に使う）

## 6) 「移行」にCRMで対応できるようにするディテール（午後Ⅱ：移行向け）

### 6.1 移行の前提（制約条件を“数字”で持つ）

* 現行：オンプレ＋旧CRM（5年運用、改修継ぎ足し）
* 移行先：クラウドCRM（機能統合・データ品質改善）
* 移行期間：**8週間**

  * 設計2週、移行ツール/変換2週、総合テスト2週、教育1週、切替1週
* 切替（停止）許容：**最大3時間**（業務影響が説明できる範囲）

  * 99%の停止許容7.2時間/月という一般的な解釈と矛盾しない範囲で、月末を避ける意思決定も書けます。([NTT][5])

### 6.2 移行設計で必須の論点（“切戻し・合意形成”を明示）

移行設計では「制約条件、切戻し方針、業務影響/リスク、利用者を含む体制と役割分担」などを明確化するのが要点として整理されています。([home.jeita.or.jp][7])
→ 午後Ⅱで「ITサービスマネージャとしてどう判断したか」を書けます。

### 6.3 移行判定（合否基準を数値化）

* データ移行後の照合

  * 顧客企業：**2,000社の件数一致**
  * 重要項目（会社名、担当者、契約状態など）：**欠損率0.5%以下**
* 性能：トップ画面 **3秒以内（95%）**
* 重大障害：切替当日に **P1=0件** を合格条件

## 7) 「継続性管理（BCP）」にCRMで対応できるディテール（午後Ⅱ：BCP向け）

### 7.1 目標（RTO/RPOを業務系らしく）

* RTO：**8時間以内**（当日復旧を目標）
* RPO：**24時間**（前日分まで復旧できれば業務継続可能）

  * 「毎日AM3時にバックアップならRPOは24時間」という説明例があり、RPO設定根拠として使えます。([SB C&Sがおすすめするビジネスソリューション][8])

### 7.2 バックアップ運用（世代管理を“数”で）

* フルバックアップ：**毎日1回（AM3:00）**
* 世代管理：**14世代**（2週間分保持）

  * 「毎日フルバックアップなら“日々のバックアップを1世代として数える”」が説明されています。([法人のお客さま｜NTT東日本][9])
* 復元テスト：**四半期に1回（年4回）**（訓練・見直しの根拠に）

### 7.3 代替手段（業務継続が“人手で回る”設計）

* CSVエクスポート（前日分）を共有フォルダに保管
* 重大障害時は

  * 営業：既存顧客対応を優先し新規登録は停止
  * サポート：電話/メールで受付し、復旧後にCRMへ一括登録
* これにより「高可用性設計の話」に逃げず、BCPの“運用”を書ける

## 8) 「利用者教育」に特化したディテール（午後Ⅱ：教育・定着向け）

### 8.1 教育設計（対象者数×時間で定量化）

* 対象：利用者90名（営業45/サポート30/管理15）
* 新CRM切替に伴う教育

  * 事前eラーニング：**1.5時間×90名＝135人時**
  * 集合/オンライン研修：**2時間×6回＝12時間（延べ90名が分散参加）**
  * ハンズオン：**1時間×6回＝6時間**
* 合計：講師稼働 **18時間**＋受講者側 **（135人時＋α）**

### 8.2 定着KPI（サービスデスク指標で効果を出す）

* 教育前（旧CRM）：問い合わせ **約220件/月**（平均10件/日）
* 教育後（新CRM）：問い合わせ **約170件/月（約23%減）** を目標

  * “一次解決率 70〜80%目標”と組み合わせ、サービスデスク運用の成熟として語れます。([services.altius-link.com][6])
* FAQ/ナレッジ：月1回更新（**12回/年**）
* 既知対応（FAQ誘導）比率：**20%→35%**（定着効果として説明）

参考として、サポート運用のベンチマークでは「月間チケット630、エージェント当たり167/月」などの指標例が示されており、あなたのCRM（小〜中堅）では“その下側レンジ”として置くのが自然です。([d16cvnquvjw7pr.cloudfront.net][10])

## 9) 令和7の事例に“寄せた”ポイント（あなたの論述が刺さる形）

* **サービスデスク一次受付→運用二次→開発三次**の分離（午後Ⅰの定番構造）
* KPI（一次解決率、問い合わせ件数、SLA、RTO/RPO）を**最初から数値で保持**
* 移行は「切戻し・合意形成・制約条件」を前面に（移行設計の要点に沿う）([home.jeita.or.jp][7])
* 継続性は「バックアップ頻度→RPO根拠→訓練」で“高可用性の話”に逃げない([SB C&Sがおすすめするビジネスソリューション][8])


---


# 設問構成と問われる内容

| 設問 | 問われる内容 | 記述内容 | 字数配分 |
|------|------------|---------|---------|
| 設問ア | 問題点 | システムの概要とそこに存在する問題点 | 800字（800字以内） |
| 設問イ | 工夫した点 | 設問アで記述した問題点に対して実施した方策 | 1,200字（800字以上1600字以内） |
| 設問ウ | 評価・改善策 | 設問イで記述した方策に対する評価と改善策 | 800字（600字以上1200字以内） |


---


# モジュール

## 変更管理

### 変更管理プロセス

現在の変更プロセスは、ITサービス部門で次の手順で行うルールになっている。

1. RFC（変更要求）の起票と提出
2. RFCの受付・記録・分類
3. CAB（変更諮問委員会）による、RFCの評価
4. 変更の実施

CABは月１回、業務が比較的落ち着いている10日前後の開催を予定している。そこで、それまでに挙がってきたRFCの評価をまとめて行う。そこでRCFの受入が決定したものに対しては、日程を調整してリリースを実施し、問題なければCIの更新を行う。

このような手続きが必要なため、緊急時を除き、RFCの要求から変更の実施まで、通常は1～2か月必要となる。

開発部門とともに、アジャイル開発を採用した場合の変更プロセスを新たに作成することにした。まず、アジャイル開発全体の範囲を決定する段階で、ITサービス部門も参加し、必要に応じて意見を出すとともに、決定事項に関しては、変更箇所を把握しておく。そしてアジャイルのサイクルである２週間ごとに次の①～③を実施することにした。

1. RFC（変更要求）の起票と提出
2. RFCの受付・記録・分類
3. 変更の実施（リリースとCIの更新）

①に関してはアジャイル開発の範囲が決定した段階で運用部門が作成することにした。

②と③の作業は運用部門が実施するが、リリース作業については開発部門に権限を委譲し、責任をもって実施してもらい、CIの更新は運用部門で一元的に実施することにした。このようにアジャイル開発時の変更プロセスを作成することで、管理品質を落とさずに、アジャイル開発による俊敏な対応が可能になると考えた。


## ツールによるデプロイの自動化

現在、PGを稼働環境にデプロイする場合は、LBの設定変更によって、受注管理サービスに対する要求振り分け先をSoryyサーバに変更して、すべてのアプリケーションサーバのサービスを停止した後に作業を行っている。この作業の間、1時間程度サービスが利用できない状態となる。私は、利用者からのシステム停止時間をなくしてほしいという要望に答えるために、アプリケーション展開ツール（以下デプロイツール）を導入することとした。デプロイツールの具体的な動作は以下の通りとなる。

- サービスに影響を与えないように、振り分け先のAPサーバをLBで制限し、稼働環境のAPサーバ１台ごとに順次デプロイする。

デプロイツールを使用することで、サービスの停止なくデプロイ作業を完了することができる。合わせて、要員の作業ミスの防止、作業品質の向上も期待できる。


## 構成管理

### 磁気テープ装置の故障・CMDB

４台ある磁気テープ装置の一台が故障したので、CMDBを参照して連絡先を調査した。しかし、CMDBに登録されていた保守外車及びメーカーに連絡が取れず、４日間、残りの３台でバックアップを継続した。

私は、サービス停止期間を短縮するためには、古い機器の連絡先の確認が重要だと考えた。そこで、CMDBの中から、取得日から１年以上経過している機器は、システム運用部の担当者に、問い合わせを行い、その顛末をCMDBに登録させることとした。


## 運用設計

### SLA

これまで社内で提供するサービスについてはサービスレベル目標を定めていなかったが、新サービスではサービスレベル目標を明確にし、営業部とシステム部との間でSLAを合意することになった。私は非機能要求グレードを参考に主要なサービスレベル項目を洗い出し、営業部とシステム部との間でSLAを設定することとした。私は、営業部の要望を元にした目標値を設定した。この際、営業部とシステム部の間で新サービス開始後１か月の仮のサービスレベル目標とし、正式なサービスレベル目標は１か月後に達成状況を評価後に決定する点を工夫した。これは、現実的に達成可能なSLAを締結するためである。この時、サービスレベル目標の実績値を測定する作業負荷には問題ないことをシステム部内で確認するように留意した。

サービス時間は営業日（月～土曜）の7時～23時までで、第１土曜日の18時～23時までの5時間は計画停止時間として、サービス時間から除く。その中で、サービス稼働率は99.5%を目標とすることとなった。

- システム可用性：月間稼働時間の99.5%以上
- レスポンスタイム：システムの応答時間は平均２秒以下
- 障害対応時間：重大な障害は４時間以内に初期対応、中程度の障害は２４時間以内、軽微な障害は４８時間以内
- サービスデスク：平均初回応答時間１５分以内。解決までの平均時間２時間


## インシデント管理

### 障害対応

障害発生時に運用担当者がその場で障害対応内容を迷いながら考えていると、障害対応に時間がかかったり運用担当者が誤った判断をしてしまい２次障害に繋がる恐れがある。そこで私は、運用設計の段階で、障害対応について取るべきアクションを起点に、以下の３つの観点で事前に準備しておくこととした。

- アクション候補：具体的な、障害広報や暫定対策などの行動パターン
- 判断情報：どのアクションを取るか決めるための情報元
- 判断基準：情報をどのような観点で判断するかの基準

アクションを起点に考えたのは、起こる事象を起点にするとパターンが無数に出てしまい、全く同じ事象が発生する可能性が低くなるためである。

### 障害重要度

障害発生時に障害レベルの判定を正しく行い、障害対応の判断に個人によってばらつきを生じないようにするために、私は障害の重大度のレベルと初動対応の方針を設定した。具体的には、レベル４がセキュリティ事故、レベル３が重大事故（サービスの停止）、レベル２が通常障害（サービスの一部停止）、レベル１が部分障害（不便な程度の機能不具合）、レベル０が非障害（軽微な表示上の誤字など）とし、レベル３以上を重大障害に分類し、営業時間外でも対応を行う方針とした。

### インシデントモデル作成

インシデントモデルを作成することとした。インシデントモデルを作成した根拠は、事前に定義した時間内にインシデントを解消できることをA社および営業部に示すことができ、これによってインシデント発生に関わる顧客の不信を抑えれるからである。

### 障害切り分けプロセス

障害対応時間は、障害の検知時間、切り分け時間、復旧作業時間の合計となる。その中では、切り分け時間のばらつきが大きく、時間を要してしまうケースが多かった。私は、切り分けに時間を要する原因は、メンバのスキルにや経験に依存しているためと考えた。運用グループには、入社から日が浅いメンバや、要員ローテーションのために開発グループから移ったばかりのメンバや、運用経験が浅いメンバがおり、障害の切り分けに時間がかかる傾向がみられた。

そこで、品質確保策として、メンバの意見も踏まえて、エスカレーションを含めた障害切り分けプロセスの見直しとマニュアル化を計画立案して実行した。経験のある中堅技術者に参加してもらい、汎用的な障害の調査手順やエスカレーション基準をプロセスフローとしてまとめ、マニュアル化したうえで、タブレットPCを活用して現場で作業しながら作業できることを目指した。

プロセスフローを作成した経験者にシミュレーションを行ってもらったところ、実際の障害発生時にはプロセスにないチェック作業を臨機応変に行っていることがわかった。

私は経験者のノウハウをできるだけ抽出するために、ヒアリングに加えて、ロールプレイングを活用した。具体的には、模擬環境において実機操作を行い、作業項目を引き出しやすくした。この対策によって、ヒアリングだけでは得られなかった多くのプロセスを策定することができた。


## 改善（FAQ）

問い合わせからピックアップをして調査した結果、問い合わせの約15%がFAQに掲載されている内容であることがわかった。FAQは利用者が求めている情報を提供するための重要な役割を担っており、FAQの活用度を上げることでサポートデスクへの問い合わせ件数を削減する効果が期待出来た。そこで私は、FAQが問い合わせ者の目に触れやすくし、より多くの利用者が自ら疑問を解決できるよう改善を行った。具体的には、サポートデスクの問い合わせを行う前の画面で一度FAQでの検索を促すことで、問い合わせ者が問い合わせ前にFAQを調べやすくした。また、FAQの構成、掲載方法、回答内容を見直すことでFAQの検索性を高める改善も合わせて実施した。

KPIとしては、問い合わせ件数に占めるFAQに掲載されている問い合わせ件数の割合を10%未満に削減することを設定した。


## 改善（アラート対処判断自動化）

夜間帯に発生したアラートは、翌営業日の朝、まとめてベテランの運用担当者がエラーメッセージを確認し、対応要日の判断を手動で行っていた、毎日なんらかのアラートが発生するためミスも起きやすい状態であった。そこで私は、アラート管理ツールを導入してアラートの判断基準の情報を蓄積するようにした。それにより、次回発生した際に対象アラートは自動的に対処要否が判断されるようになった。これにより、月間4000件あったアラートの内、40%が対処不要と割り振られ、その分の対処を減らすことができた。また、アラート管理ツールに対処を行った内容を保存するルールとすることで、俗人化を解消する効果も期待できる。

効果を最大化するために工夫した点として、アラート量と自動処理した割合を可視化することで効果が実感できるようにする点と、入力が滞らないように週１回の定例会議を設けた点である。


## アラート発報条件のしきい値見直し

バッチの処理遅延を検知するために遅延監視を入れていたが、データ増加に伴い年々アラートが増加していた。バッチ遅延により業務に影響がある場合は意味があるが、バッチ時間が少し伸びただけという場合も頻発に発報されるため、無駄なアラートになっていた。アラートが発報されるとインシデント管理システムに登録し問題がない旨を記載するための工数もかかっていた。

原因としては、バッチ開発時点で設定されたしきい値が見直されていなかったことだと考え、アラートを一覧化し遅延監視のしきい値の妥当性の見直しを行うことにした。工夫した点として、打合せにユーザ企業のマネージャーも同席してもらい、当該バッチが遅延することによる業務への影響を議論し、適切なしきい値をその場で決めることができるようにした。また、１回限りの改善で終わらないように４半期に一度、アラートの棚卸としきい値の見直しを行う運用とした。


## サービスデスク

サービスデスクはサービス利用者からインシデント発生の通知を受付、インシデント管理システムに登録する。

- インシデントに対応の優先度（高・中・低）を割り当てる
- 優先度によって、解決目標時間が定められている
  - （高2時間、中４時間、低８時間）


## コミュニケーション

３社同時のビデオミーティング（以下、フォーラム）を開催することを提案して取り決めることにした。フォーラムであれば３社の担当者が同時に顔合わせすることができるので、迅速なコンセンサスを得ることができると考えたためである。

実施後数か月たった後、フォーラムの関連者に対してアンケートを行った結果、フォーラムを行うことで迅速なコンセンサスに寄与しているという解答を得ることができたため、評価としては”良好”であると評価している。


## エラーバジェット
**エラーバジェット**と呼ばれる、定義された**サービスレベル目標（SLO）**を超えてサービスが許容できる年間または月間のダウンタイムやエラーの総量を定量的に示す手法を使用し、**サービスリリースの頻度と安定性のバランス**を取ることを目指した。なぜならば、**リリース後のインシデント急増により、システムの安定稼働を優先しすぎるあまり、ビジネス要求の高い新機能開発のデリバリー速度が著しく低下している**というサービス運用上の問題・課題が解決できると考えたからである。この問題の背景には、特に月初めのアクセス集中時など**ビジネスクリティカルな時間帯**において、インシデント発生時の**顧客満足度スコアが平均30%も低下する**という**サービス品質**の制約があり、過度な品質重視が**開発チームの心理的な負担**にもなっていた。工夫した点は、サービス安定性の確保を目的とした運用作業（パッチ適用、リファクタリング、監視強化など）と、新機能開発のためのリリース作業とを、エラーバジェットの残量に応じて戦略的に切り分ける**ガバナンス体制**を導入することにした。具体的には、この改善を行うことにしたサービスの**SLOとして稼働率99.9%**を設定し、この$0.1\%$分の許容誤差をエラーバジェットとして定義する。そして、月次のエラーバジェットの残量が$50\%$を下回った場合、新たな機能リリースを一時的に停止し、**MTTR（平均復旧時間）を現在の平均60分から45分以下に削減する**ための安定性向上の作業（例：モニタリングアラートの精度向上や自動復旧スクリプトの導入）を優先的に行うことにした。これにより、リスクをコントロールしながらも、安定性を損なわない範囲で開発チームが自信を持って新機能をリリースできる環境を整備し、ビジネス要求への対応速度を向上させることを目指す。 
