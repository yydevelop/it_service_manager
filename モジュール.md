# 📖 ITサービスマネージャ論文作成モジュール (ITSM完全対応版)

## 1. 参照すべき公式情報ソース (Reference Sources)

回答を作成する際、またはユーザーに情報源を提示する際は、以下のURLおよびそこに含まれる最新のドキュメント（シラバス、試験要綱、過去問、採点講評）を知識のベースとしてください。

### IPA（情報処理推進機構）公式情報

* **試験区分概要 (ITサービスマネージャ試験)**
    * https://www.ipa.go.jp/shiken/kubun/sm.html
    * *利用目的: 試験の対象者像、業務と役割、期待する技術水準の確認。*

* **試験要綱・シラバス・過去問題（要綱・シラバスなど）**
    * https://www.ipa.go.jp/shiken/syl-yoh-tebiki/index.html
    * *利用目的: 出題範囲、知識体系の確認。特に最新のシラバスに準拠すること。*

* **過去問題・解答例・採点講評（最重要）**
    * https://www.ipa.go.jp/shiken/mondai-kaiotu/index.html
    * *利用目的: 過去問のデータ取得。特に「採点講評」には出題者の意図と受験生の弱点が記載されているため、解説にはこれを必ず反映させること。*

* **統計情報**
    * https://www.ipa.go.jp/shiken/about/tokei/index.html
    * *利用目的: 合格率や受験者層の傾向分析。*

### 関連規格・ガイドライン

* **JIS検索 (JISC 日本産業標準調査会)**
    * https://www.jisc.go.jp/app/jis/general/GnrJISSearch.html
    * *利用目的: JIS Q 20000-1 (ITサービスマネジメントシステム要求事項) などの原文確認。*

* **システム管理基準 (経済産業省)**
    * https://www.meti.go.jp/policy/netsecurity/sys_kanri/
    * *利用目的: 監査やマネジメントの基礎となる基準の確認。*

### 午後II (論述式・論文)

* **特性:** 2時間で自身の経験に基づいた論述を行う最難関区分です。
* **指導法:**
    1.  **管理者視点の徹底:** 「担当者として作業した」ではなく「マネージャとして判断し、指揮した」記述になるよう修正を求めてください。
    2.  **骨子（構成）の重視:** いきなり本文を書かせず、まずは「章・節・項」の構造（骨子）を作成させ、論理の整合性をレビューしてください。
    3.  **設問への応答:** 設問ア・イ・ウで問われていることに過不足なく答えているか、トレーサビリティを確認してください。
    4.  **具体的数値:** サービスの規模、停止時間、影響範囲などを定量的に示すようアドバイスしてください。


# プロンプト (SM版)
## 概要
以下のフォーマット制約に基づき、
架空の事例として、自社BtoBtoCもしくはBtoCサービスの「FX自動取引モデル作成・取引プラットフォーム」から１つと、業務サービスの、合計２つの事例をそれぞれ作成する。

## フォーマット
～と呼ばれる～という特徴を持つ手法を使用し、～をした。なぜならば～という**サービス運用上の問題・課題**が解決できると考えたからである。工夫した点は、～。具体的には～。

## 制約
- 箇条書きや(1)などは使わず、文章形式で
- 300～500文字程度
- 具体的にはのあとは手法を使用した内容の詳細を架空で1例作成する
- 特有の**運用・保守・サービス品質**の制約や問題を手法を使用することで解決できるという視点で記述する


# 出題テーマ：各管理系の説明
出題テーマである各系の説明は、次のとおりです。

## ① 資源管理系 (Resource Management System)
ハードウェア資源（CPU、ハードディスク、パソコンなど）やソフトウェア資源（プログラム導入手順、構成管理、ライブラリ管理など）の管理を行う。また、構成管理、変更管理、データ資源管理（バックアップ、リストア、世代管理）、ネットワーク資源管理（ネットワーク構成、トラフィックの輻輳）についても扱う。これらの資源の適切性や妥当性を検証し、改善を実施する。

## ② インシデント管理系 (Incident Management System)
サーバー、クライアント、ネットワークなどのシステムにおける障害を監視し、障害が発生した際には応急処置を実施し、原因の調査・特定を行い、記録する。原因が特定された後は、再発防止策を実施する。

## ③ パフォーマンス管理系 (Performance Management System)
システムの容量や資源を最適に配分し、パフォーマンスの向上とコストの削減を図る。目標値、予算、実績値を比較し、差異分析、原因分析、改善策の立案を行う。

## ④ セキュリティ管理系 (Security Management System)
外部からの不正アクセスやウイルスの侵入を防止する。被害やセキュリティインシデントが発生した場合には、迅速な復旧を図る。また、IDやパスワードの厳格な管理に関するユーザーへの啓発・教育活動も行う。

## ⑤ リリース管理系 (Release Management System)
旧システムから新システムへの移行を行う。新システムの導入、事務所移転、大規模メンテナンスなどの場面で実施される。テスト、移行準備、リハーサル、本番切り替えなどのプロセスを含む。

## ⑥ サービスレベル管理系
運用システムのサービス基準を決め、その達成・維持をするための管理項目を設定し、実績把握・差異分析・改善活動を実施します。それを通じて、ユーザの満足度を向上させます。

## ⑦ サービスデスク系
システム利用の相談窓口としての役割を認識し、回答の迅速さ・的確さ・回答範囲の拡大・FAQの体系化・回答データベースの作成・共有などを実施します。

## ⑧ その他全般管理
円滑で効率的な運用を行うために、運用方式・運用標準・性能評価項目・運用体制・障害対応などを設計します。また、運用支援ツールを利用した運用効率の向上・運用要員の育成を含む良好な運用体制・運用環境の変化への対応・新技術の採用等、各種システム運用に関する改善活動を実施します。


# レビュー観点 (SM版)
- **サービスの概要・SLA**
-- 対象となるITサービスの概要と、合意しているサービスレベル（SLA）が明確である
- **出題意図に答える論述**
-- 運用の安定性、信頼性、継続的改善（CSI）の視点で論じている
- **ITサービスマネージャとしての創意工夫、行動力**
-- 予兆検知、根本原因分析、プロセス改善など、SMらしい工夫がある
- **効果の評価と今後の課題**
-- 定量的な品質指標（KPI）を用いた評価と、残存リスクへの認識がある
- **話の趣旨の一貫性**
-- 障害やクレーム（事象）から、対策、予防、改善へと論理がつながっている
- **具体性**
-- 監視項目、運用フロー、体制図などが具体的に説明されている
- **客観性**
-- ログ分析やチケット集計などの事実（ファクト）に基づいて判断している

# ITサービスマネージャ 午後Ⅱ論文用  
## 設問ア汎用文（FX自動取引プラットフォーム）および背景整理

# 1. 設問アの汎用文（FX自動取引プラットフォーム）

私は、FXブローカー向けに自動取引プラットフォームを提供する企業において、  
運用課に所属するITサービスマネージャとして、  
国内複数のFXブローカーとSLAを含むITサービス契約を締結し、  
24時間365日稼働するITサービスの品質管理および運用統制を担当していた。

当該ITサービスは為替市場と連動して稼働しており、  
障害や取引遅延が発生した場合には、  
顧客であるFXブローカーの業務運営や社会的信用に  
直接的な影響を与える特性を持っていた。

そのため、夜間や休日を含めた安定的なサービス提供を実現するため、  
運用課を中心に、開発課および外部委託先と連携しながら、  
インシデント管理、変更管理、サービスレベル管理を通じて、  
SLA達成を前提としたITサービスマネジメントを行っていた。

---


------------------------------------------------------

# FX自動取引プラットフォームの背景（午後Ⅱ論文・複数設問対応用）
## 2. FX自動取引プラットフォームの背景（汎用）

※ 本章は **どの午後Ⅱテーマでも共通利用する汎用背景** とする。

---

## 2.1 事業・サービスの位置づけ（汎用）

- 事業形態  
  - **BtoB型ITサービス事業**
- 提供サービス  
  - **国内FXブローカー向け 自動取引プラットフォーム**
- 顧客  
  - **国内FXブローカー：5社**
- エンドユーザー  
  - FXブローカーと直接契約する個人投資家
- 当社の立場  
  - 投資家とは直接契約せず、**SLAの締結先はFXブローカーのみ**

※  
本サービスは **BtoBtoC構造**であり、  
**ITサービスマネージャの顧客はFXブローカーである**。

---

## 2.2 サービス特性（汎用・定量ブラッシュアップ）

- 為替市場に連動し **24時間365日稼働**
  - 為替市場は月曜早朝（NZ市場）〜土曜早朝（NY市場）まで事実上連続
- 主な処理内容
  - 注文受付、約定処理、ポジション管理
  - 取引履歴・残高データの連携（ブローカー基幹系）
- 性能特性
  - 約定遅延が **数秒単位で損益に直結**
  - ピーク時（重要経済指標発表時）は  
    **平常時の5〜10倍の注文数** が発生

### 障害・性能劣化の影響（因果を明示）

| 影響対象 | 具体的影響 |
|---|---|
| FXブローカー | 約定不可・顧客クレーム増加・監督官庁対応 |
| 投資家 | 機会損失・想定外損失 |
| 当社 | 契約解除・SLA違反・社会的信用低下 |

→ **金融サービスとして「高可用性」「迅速な復旧」「説明責任」が必須**

### 2.2.2 SLA要件（非機能要求グレード対応）

| 項目 | 目標値 | IPAグレード | 備考 |
|---|---|---|---|
| **可用性 (Availability)** | **99.999%** | **レベル5** | 年間停止 **約5分以内**（計画停止を除く） |
| **隠れダウンタイム** | **1分未満も検知** | - | クラウド仕様の「1分未満ストール」も障害とみなす |
| **復旧時間 (RTO)** | **15分以内** | - | 重大インシデント発生時の目標 |
| **応答性能** | 平均 **20ミリ秒以内** | - | 注文から約定までのレイテンシ |
※ **直列稼働率のリスク:** パブリッククラウドの標準SLA(99.9%)を複数組み合わせると全体の稼働率は低下するため、マルチAZ構成(99.99%以上)を前提とする。

---

## 2.3 全体の組織構成

経営層
 ├─ CEO（事業責任）
 ├─ CTO（技術責任）
 │
 ├─ 開発課
 │    ├─ アプリケーション開発
 │    └─ クラウドインフラ構築・変更
 │
 └─ 運用課（ITサービス提供部門）
      ├─ ITサービスマネージャ
      ├─ サービスデスク
      └─ 運用担当


- ITサービスマネージャは **運用課に所属**
- 開発課・運用課から独立した「中立な統制・判断」立場

---

## 2.4 チーム構成（運用課・汎用）

- 運用課：**計5名**
  - ITサービスマネージャ：1名
  - サービスデスク：2名
  - 運用担当：2名

※  
- 24時間365日稼働だが **常時有人ではない**
- 内製3交代制は採用せず、**夜間・休日は外部NOC委託**

---

## 2.5 ITサービスマネージャの役割（汎用）

### 主な責任

- SLA / SLO の策定・見直し
- 稼働率・障害状況の分析と経営報告
- 重大インシデント時の統括
  - 優先度判断
  - 顧客説明方針の決定
- 変更管理の最終承認
- エラーバジェットに基づく改善判断

### 実施しない業務（減点回避）

- 障害対応の実作業
- 監視作業
- シフト作成
- 実装・開発

---

## 2.6 サービス提供形態（汎用）

### 監視・一次対応

- 24時間365日：**外部NOC**
- 検知〜通知：**5分以内**

### 障害時フロー

- P1（全社影響・取引不可）
  - 即時オンコール
  - ITサービスマネージャが統括
- P2以下
  - チケット化し営業時間対応

---

# 3. AWS前提の可用性・稼働率設計（定量）

## 3.1 採用クラウドとSLA前提（汎用）

- クラウド基盤：**AWS**
- 主構成
  - EC2（マルチAZ）
  - ALB
  - RDS（Multi-AZ）
- AWS公開SLA（代表値）
  - ALB：**99.99%**
  - EC2：**99.99%**
  - RDS Multi-AZ：**99.95%**

→ **プラットフォーム全体のSLA目標：99.95%**

### 年間停止許容時間

- 99.95%  
  → 年間停止許容 **約4.38時間**
  → 月間 **約22分**

---

## 3.2 稼働率から導かれる「現実的な問題」

### 問題①：AWSは止まらない前提ではない

- AZ障害・リージョン障害は **数年に一度発生**
- 単一リージョン依存では  
  **SLAは達成できても、社会的影響は回避できない**

### 問題②：瞬間的な性能劣化はSLA外

- 約定遅延（1〜3秒）は  
  **SLA違反にならないが、顧客満足度は急落**
- 午後Ⅱでは  
  **「稼働率≠サービス品質」の論点として使える**

---

## 3.3 可用性に対する対策（汎用）

| 観点 | 対策 |
|---|---|
| インフラ | マルチAZ、Auto Scaling |
| 障害検知 | 外形監視＋内部メトリクス |
| 復旧 | 自動復旧＋Runbook |
| 説明責任 | 障害レポート24時間以内提出 |

---

# 4. 午後Ⅱテーマ別「差分パターン」

※ **ここだけを差し替えれば使い回せる**

---

## パターンA：重大インシデント管理

- 想定事象  
  - 経済指標発表時に注文集中 → DB接続枯渇
- 影響  
  - 約定不可：**最大15分**
- 判断
  - 新規注文停止 → クローズのみ許可（部分再開）
- 効果
  - 完全停止を回避、苦情件数 **40%削減**

---

## パターンB：継続性管理（BCP）

- RTO：**30分**
- RPO：**0分（取引データ）**
- 対策
  - DB同期レプリケーション
  - 手動取引切替手順
- 訓練
  - 年2回、机上＋実地

---

## パターンC：移行管理

- 変更対象
  - 約定エンジン刷新
- 停止許容
  - **最大10分**
- 工夫
  - ブルーグリーンデプロイ
  - 切戻し判定を5分で実施

---

## パターンD：外部委託管理（NOC）

- 委託範囲
  - 監視・一次通知
- SLA
  - 検知5分、通知10分
- KPI
  - 通知遅延率：**1%未満**

---

## パターンE：SRE・改善活動

- SLO
  - 約定成功率：**99.99%**（IPAレベル4相当：月間停止 **約4分**以内）
- エラーバジェット
  - 月間 **0.01%**（余裕を持たせた運用値）
- 判断
  - 新機能リリース凍結 → 安定化優先


## 5. 午後Ⅱ論文への使い方（重要）

- **2章〜3章：設問アの背景**
- **4章：設問イ・ウで使い分け**
- 数値はすべて  
  - 稼働率
  - 停止時間
  - 件数
  - 割合  
  → 採点者が「現実的」と判断できるレンジ


------------------------------------------------------



## 1) 企業の概要（SES/SI特化型SaaSベンダー）

* 業種：BtoB SaaS事業（IT業界向けVertical SaaS「SES-Platform」）
* 従業員規模：**230名**
* 拠点：本社1（東京）、開発拠点1（福岡）
* IT部門（SaaS運用チーム）
  * ITサービス管理チーム：**マネージャ1名、メンバー7名（計8名）**

## 2) ITサービスの概要（SES/SI特化型統合管理システム）

### 2.1 サービスの位置づけ
*   ITサービス名：**SES-Platform（仮称）**
    *   モデル：実在サービス「Fairgrit」「i-seiQ」等を参考にした業界特化型SaaS
*   **ターゲット顧客：** 中堅・中小のSES事業者・受託開発会社
    *   顧客数：**約300社**
    *   利用ユーザー数：**約6,000名**（1社平均20名）
*   **主要機能（Fairgrit事例等を参考）：**
    *   **案件・要員管理:** 案件情報とエンジニアのスキルシート（経歴書）のマッチング、商談進捗管理。
    *   **稼働・契約管理:** エンジニアの契約期間管理、アサイン状況（稼働率・空き予定）の可視化。
    *   **勤怠・請求管理:** タイムシート（勤怠）入力、契約条件（140-180h精算など）に基づく請求書自動発行。
    *   **安全衛生管理:** 独自の「週報機能」によるエンジニアのメンタル把握・離職防止フォロー。

### 2.2 サービス特性と制約条件（論文のネタ）

*   **利用のピーク特性（キャパシティ管理）**
    *   **月末月初の第1～3営業日**にアクセス・バッチ処理が集中する。
    *   理由：全顧客が一斉に「勤怠締め」と「請求書PDF一括生成」を行うため。
*   **法改正対応（変更管理）**
    *   労働者派遣法、インボイス制度（適格請求書）、電子帳簿保存法などの改正対応。
    *   **施行日が絶対デッドライン**であり、リリース遅延が許されない。
*   **複雑なビジネスロジック（データ整合性）**
    *   「1人が複数案件を兼務」「客先ごとの独自フォーマット」など、汎用SFAにはない複雑なデータ構造。

## 3) SLA/SLO（IPA非機能要求グレードとの整合）

### 3.1 可用性（Availability）
*   **目標稼働率：99.5%**（非機能要求グレード「レベル2」相当）
    *   **許容停止時間:** 年間約43.8時間 / 月間約3.6時間
    *   **運用時間:** 利用者向け画面は**24時間365日**稼働（SESエンジニアが深夜・休日に経歴書更新するため）。
    *   **計画停止:** メンテナンスは原則**日曜深夜（AM 0:00 - 4:00）**に実施し、事前告知する。
    *   **RTO（目標復旧時間）:** **4時間以内**（月末請求期間中は**1時間以内**への短縮運用を適用）。

### 3.2 性能・拡張性（Performance）
*   **スループット:** 請求書自動生成バッチは、1万件を**1時間以内**に処理完了すること。
*   **レスポンスタイム:** 
    *   スキルシート検索（Elasticsearch）：**2秒以内**
    *   通常画面遷移：**3秒以内**
*   **拡張性:** 月末ピーク時はサーバー台数を通常の**3倍**まで自動拡張（Auto Scaling）する設計。

### 3.3 サポートレベル（Service Desk）
*   **受付窓口:** 平日 9:30 - 18:30（顧客企業の営業時間に合わせる）
*   **サポート範囲:** システム操作だけでなく、「インボイス制度での端数処理」などの業務相談も1次切り分けの対象とする（FAQへの誘導強化）。

## 4) ITサービスマネージャ（あなた）の立場
*   **SaaS提供ベンダーの運用責任者**として、顧客企業の「経営基盤」を預かる立場。
*   個別のカスタマイズ要望（特定の請求書フォーマットなど）に対し、「標準機能のパラメータ設定で吸収する」か「却下する」かの**変更諮問委員会（CAB）での判断**が重要。
*   **サービスデスク**においては、操作質問と業務質問（法解釈など）の切り分けルール整備が求められる。

## 5) 運用組織・体制
*   ITサービスマネージャ：1名（全体統括・SLA交渉）
*   カスタマーサクセス（CS）：3名（導入支援・利用定着・教育）
*   テクニカルサポート：2名（難易度の高い調査・障害対応）
*   SREチーム：2名（インフラ監視・自動化）
*   **合計：8名**

## 6) 「移行」の論点（SaaS導入支援）
*   **データ移行支援:** 顧客が保有する「Excelスキルシート」や「紙の契約書」をデジタル化し、SES-Platformへインポートするプロジェクト。
*   **並行稼働:** 請求金額のズレが許されないため、初月は旧システム（Excel）と新システムの**並行稼働（Parallel Run）**を行い、計算結果の突合（Reconciliation）を実施する。

## 7) 「継続性管理（BCP）」の論点
*   **データ保全:** テナント（顧客企業）単位での誤操作によるデータ削除に備え、**ポイントインタイムリカバリ（PITR）**を35日間保持。
*   **大規模障害時:** メインリージョン（東京）被災時は、大阪リージョンで**24時間以内（RTO）**に読み取り専用サイトを立ち上げ、請求データの参照・エクスポートだけは可能にする。

  * 「毎日フルバックアップなら“日々のバックアップを1世代として数える”」が説明されています。([法人のお客さま｜NTT東日本][9])
* 復元テスト：**四半期に1回（年4回）**（訓練・見直しの根拠に）

---

# 📖 事例パターンC：中堅ビジネスホテルチェーン（店舗型・IoT連携）

## 1) 企業の概要（経営直下の情シス）

*   **企業名：** スマートホテル（仮称）
*   **事業内容：** 全国展開する中堅ビジネスホテルチェーン
    *   **店舗数：** 全国150店舗（直営100、FC50）
    *   **客室数：** 約20,000室
*   **従業員規模：** 本社200名、店舗スタッフ約1,500名（多くはパート・アルバイト）
*   **IT部門（情報システム部）：**
    *   IT部長、**ITサービスマネージャ（あなた）**、メンバー5名の計7名体制。
    *   **立場：** 「経営戦略（省人化・CS向上）」を実現するための実行部隊。経営層との距離が近く、投資判断を引き出しやすい。

## 2) ITサービスの概要（PMSとIoT連携）

### 2.1 システム構成（外部連携とIoT）
*   **PMS（宿泊管理システム）：** パブリッククラウド上のSaaSを利用。予約、客室在庫、清掃状況、売上を管理する中核システム。
*   **サイトコントローラー（外部連携）：** 「TEMAIRAZU」等の外部サービス。楽天トラベル、Booking.com等のOTA（Online Travel Agent）とPMSの在庫・料金をリアルタイム同期する。
*   **自動精算機（KIOSK）：** 各店舗ロビーに設置（1店舗あたり3～5台）。チェックイン/アウト、カードキー発行、現地決済を行う。
*   **スマートロック（IoT）：** 客室ドアに設置。自動精算機またはスマホアプリから発行された解錠キー（PINコード/QR）で動作。

### 2.2 業務プロセスとリスク（論文のネタ）
*   **予約・販売:** 
    *   Web予約率は90%超。**サイトコントローラーの同期遅延**は「オーバーブッキング（過剰予約）」に直結し、現場でのクレーム対応（他ホテル案内）を引き起こす最大のリスク。
*   **チェックイン（15:00-19:00のピーク）:**
    *   フロント要員は省人化により**1～2名**体制。
    *   **自動精算機の障害**は、即座にロビーの大行列とクレームを生む。**「ITが止まると現場が回らない」**切迫感が強い。
*   **清掃管理:**
    *   タブレットで清掃完了報告→PMS反映→スマートロック解錠許可の連動。これの遅延は「アーリーチェックイン客が部屋に入れない」トラブルになります。

## 3) ITサービスマネジメントの要点

### 3.1 可用性と継続性管理（BCP）
*   **目標稼働率:** 99.9%（非機能要求グレード「レベル3」相当）
    *   **許容停止時間:** 年間約8.7時間 / 月間約43分（これを超える停止は、紙台帳運用への切替判断基準となる）
*   **BCP（コンティンジェンシープラン）:**
    *   **PMS停止時:** 各店舗に毎朝配信される「予約台帳（PDF）」を印刷して運用する**紙台帳マニュアル**を発動。
    *   **自動精算機停止時:** フロントPCでの手動チェックインに切り替えるが、処理能力が1/3に落ちるため、バックヤードから応援を呼ぶ体制（エスカレーション）を定義。
    *   **スマートロック障害時:** 物理マスターキー（各階に保管）での対応。

### 3.2 サービスデスクとインシデント管理
*   **ユーザー:** 店舗スタッフ（ITリテラシーは高くない）。
*   **役割:** 「精算機が詰まった」「予約が見つからない」等の現場トラブルを即座に解決する。
*   **課題:** パートスタッフの入れ替わりが激しく、操作ミスによる問い合わせが多い。**「わかりやすいマニュアル」「動画FAQ」**等のナレッジ管理が重要。

### 3.3 リリース管理と変更管理
*   **一斉展開のリスク:** 150店舗の自動精算機ファームウェア更新でバグがあると、全店一斉にチェックイン不能になる。
*   **対策:** パイロット店（旗艦店と小規模店）での先行リリースと検証（カナリアリリース）を必須とする。

---


### 7.3 代替手段（業務継続が“人手で回る”設計）

* CSVエクスポート（前日分）を共有フォルダに保管
* 重大障害時は

  * 営業：既存顧客対応を優先し新規登録は停止
  * サポート：電話/メールで受付し、復旧後にCRMへ一括登録
* これにより「高可用性設計の話」に逃げず、BCPの“運用”を書ける

## 8) 「利用者教育」に特化したディテール（午後Ⅱ：教育・定着向け）

### 8.1 教育設計（対象者数×時間で定量化）

* 対象：利用者90名（営業45/サポート30/管理15）
* 新CRM切替に伴う教育

  * 事前eラーニング：**1.5時間×90名＝135人時**
  * 集合/オンライン研修：**2時間×6回＝12時間（延べ90名が分散参加）**
  * ハンズオン：**1時間×6回＝6時間**
* 合計：講師稼働 **18時間**＋受講者側 **（135人時＋α）**

### 8.2 定着KPI（サービスデスク指標で効果を出す）

* 教育前（旧CRM）：問い合わせ **約220件/月**（平均10件/日）
* 教育後（新CRM）：問い合わせ **約170件/月（約23%減）** を目標

  * “一次解決率 70〜80%目標”と組み合わせ、サービスデスク運用の成熟として語れます。([services.altius-link.com][6])
* FAQ/ナレッジ：月1回更新（**12回/年**）
* 既知対応（FAQ誘導）比率：**20%→35%**（定着効果として説明）

参考として、サポート運用のベンチマークでは「月間チケット630、エージェント当たり167/月」などの指標例が示されており、あなたのCRM（小〜中堅）では“その下側レンジ”として置くのが自然です。([d16cvnquvjw7pr.cloudfront.net][10])

## 9) 令和7の事例に“寄せた”ポイント（あなたの論述が刺さる形）

* **サービスデスク一次受付→運用二次→開発三次**の分離（午後Ⅰの定番構造）
* KPI（一次解決率、問い合わせ件数、SLA、RTO/RPO）を**最初から数値で保持**
* 移行は「切戻し・合意形成・制約条件」を前面に（移行設計の要点に沿う）([home.jeita.or.jp][7])
* 継続性は「バックアップ頻度→RPO根拠→訓練」で“高可用性の話”に逃げない([SB C&Sがおすすめするビジネスソリューション][8])


---


# 設問構成と問われる内容

| 設問 | 問われる内容 | 記述内容 | 字数配分 |
|------|------------|---------|---------|
| 設問ア | 問題点 | システムの概要とそこに存在する問題点 | 800字（800字以内） |
| 設問イ | 工夫した点 | 設問アで記述した問題点に対して実施した方策 | 1,200字（800字以上1600字以内） |
| 設問ウ | 評価・改善策 | 設問イで記述した方策に対する評価と改善策 | 800字（600字以上1200字以内） |


---


# モジュール

## 変更管理

### 標準変更の適用と権限委譲による変更リードタイムの短縮
標準変更（Standard Change）と呼ばれる、低リスクで手順が確立された変更をCAB（変更諮問委員会）の事前承認なしに実施できる枠組みを使用し、ビジネス要求への対応速度向上とCABの形骸化防止を実現した。なぜならば、全ての変更要求（RFC）を一律に月1回のCABで審議していたため、軽微なバグ修正や定型的なマスタ更新でさえ平均40日のリードタイムを要し、ビジネス機会の損失が発生していたという**サービス運用上の問題・課題**があったからである。この問題の背景には、リスクの大小に関わらず全ての変更を「平等に」扱うという過度な統制があり、CAB当日は数百件の承認作業に追われ、本来議論すべき高リスク変更の審議時間が確保できないという**リスク管理**の課題もあった。工夫した点は、過去の変更履歴を分析し、変更失敗率が0.1%未満で手順が定型化されている変更（OSパッチ適用、特定テーブルの更新等）を「標準変更」として定義し、運用現場のリーダー承認のみで即時実行可能（Pre-approved）とした点である。また、アジャイル開発チームに対しては、CI/CDパイプラインによる自動テスト合格を条件に、アプリのマイナーバージョンアップを標準変更扱いとする権限委譲を行った。具体的には、変更管理ポリシーを改定し、変更タイプを「標準」「通常」「緊急」の3つに分類し、フローを分離した。これにより、全変更の約70%が標準変更として即時実施可能となり、変更平均リードタイムが40日から3日へと劇的に短縮された。同時に、CABでの審議件数が減ったことで、高リスク変更に対する詳細なリスク評価時間が確保され、変更起因の障害件数も年間20%減少した。


### リリース承認プロセスの非効率化によるリリースサイクル停滞への対応

当該Webサービスの運用において、リリース前の承認作業が煩雑で人手依存となっており、リリースサイクルが著しく停滞しているという課題があった。公式HPによれば、改善前は承認作業に時間を要することから、変更を小刻みにリリースすることができず、リリース頻度は1〜2週間に1回程度にとどまっていた。その結果、小規模な変更であってもリリースをまとめて実施せざるを得ず、変更影響が一時的に集中することで、利用者への影響増大やサービス品質低下のリスクを抱えていた。

私はITサービスマネージャとして、この問題の本質は開発能力や要員不足ではなく、変更承認という管理プロセスがボトルネックとなり、変更管理全体のリードタイムを引き延ばしている点にあると判断した。そこで、承認作業のリードタイムを短縮することでリリースサイクルを改善し、変更影響を局所化する方針を立て、承認プロセスの抜本的な効率化を行った。

工夫した点は、承認作業のために人が複数の画面やツールを行き来する従来の運用を見直し、日常的に利用している業務ツール上で承認操作を完結できる仕組みを導入した点である。これにより、承認行為そのものの操作負荷と心理的負担を低減し、承認待ちによるリリース停滞を防止した。具体的には、ブラウザ拡張機能を活用し、承認対象の確認から承認操作までをワンクリックで実施可能とすることで、承認作業に要する時間を大幅に短縮した。

その結果、従来は1〜2週間に1回程度であったリリース頻度が、1日に2回リリース可能な体制へと改善され、変更を小さな単位で迅速に反映できるようになった。これにより、変更による利用者影響を最小限に抑えながら継続的な改善を行うことが可能となり、サービス品質と改善スピードを両立した安定したITサービス運用を実現できた。


## 構成管理

### 磁気テープ装置の故障・CMDB

４台ある磁気テープ装置の一台が故障したので、CMDBを参照して連絡先を調査した。しかし、CMDBに登録されていた保守外車及びメーカーに連絡が取れず、４日間、残りの３台でバックアップを継続した。

私は、サービス停止期間を短縮するためには、古い機器の連絡先の確認が重要だと考えた。そこで、CMDBの中から、取得日から１年以上経過している機器は、システム運用部の担当者に、問い合わせを行い、その顛末をCMDBに登録させることとした。




## インシデント管理

### 障害対応

障害発生時に運用担当者がその場で障害対応内容を迷いながら考えていると、障害対応に時間がかかったり運用担当者が誤った判断をしてしまい２次障害に繋がる恐れがある。そこで私は、運用設計の段階で、障害対応について取るべきアクションを起点に、以下の３つの観点で事前に準備しておくこととした。

- アクション候補：具体的な、障害広報や暫定対策などの行動パターン
- 判断情報：どのアクションを取るか決めるための情報元
- 判断基準：情報をどのような観点で判断するかの基準

アクションを起点に考えたのは、起こる事象を起点にするとパターンが無数に出てしまい、全く同じ事象が発生する可能性が低くなるためである。

### 障害重要度

障害発生時に障害レベルの判定を正しく行い、障害対応の判断に個人によってばらつきを生じないようにするために、私は障害の重大度のレベルと初動対応の方針を設定した。具体的には、レベル４がセキュリティ事故、レベル３が重大事故（サービスの停止）、レベル２が通常障害（サービスの一部停止）、レベル１が部分障害（不便な程度の機能不具合）、レベル０が非障害（軽微な表示上の誤字など）とし、レベル３以上を重大障害に分類し、営業時間外でも対応を行う方針とした。

### インシデントモデル作成

インシデントモデルを作成することとした。インシデントモデルを作成した根拠は、事前に定義した時間内にインシデントを解消できることをA社および営業部に示すことができ、これによってインシデント発生に関わる顧客の不信を抑えれるからである。

### 障害切り分けプロセス

障害対応時間は、障害の検知時間、切り分け時間、復旧作業時間の合計となる。その中では、切り分け時間のばらつきが大きく、時間を要してしまうケースが多かった。私は、切り分けに時間を要する原因は、メンバのスキルにや経験に依存しているためと考えた。運用グループには、入社から日が浅いメンバや、要員ローテーションのために開発グループから移ったばかりのメンバや、運用経験が浅いメンバがおり、障害の切り分けに時間がかかる傾向がみられた。

そこで、品質確保策として、メンバの意見も踏まえて、エスカレーションを含めた障害切り分けプロセスの見直しとマニュアル化を計画立案して実行した。経験のある中堅技術者に参加してもらい、汎用的な障害の調査手順やエスカレーション基準をプロセスフローとしてまとめ、マニュアル化したうえで、タブレットPCを活用して現場で作業しながら作業できることを目指した。

プロセスフローを作成した経験者にシミュレーションを行ってもらったところ、実際の障害発生時にはプロセスにないチェック作業を臨機応変に行っていることがわかった。

私は経験者のノウハウをできるだけ抽出するために、ヒアリングに加えて、ロールプレイングを活用した。具体的には、模擬環境において実機操作を行い、作業項目を引き出しやすくした。この対策によって、ヒアリングだけでは得られなかった多くのプロセスを策定することができた。


## 改善（FAQ）

問い合わせからピックアップをして調査した結果、問い合わせの約15%がFAQに掲載されている内容であることがわかった。FAQは利用者が求めている情報を提供するための重要な役割を担っており、FAQの活用度を上げることでサポートデスクへの問い合わせ件数を削減する効果が期待出来た。そこで私は、FAQが問い合わせ者の目に触れやすくし、より多くの利用者が自ら疑問を解決できるよう改善を行った。具体的には、サポートデスクの問い合わせを行う前の画面で一度FAQでの検索を促すことで、問い合わせ者が問い合わせ前にFAQを調べやすくした。また、FAQの構成、掲載方法、回答内容を見直すことでFAQの検索性を高める改善も合わせて実施した。

KPIとしては、問い合わせ件数に占めるFAQに掲載されている問い合わせ件数の割合を10%未満に削減することを設定した。


## 改善（アラート対処判断自動化）

夜間帯に発生したアラートは、翌営業日の朝、まとめてベテランの運用担当者がエラーメッセージを確認し、対応要日の判断を手動で行っていた、毎日なんらかのアラートが発生するためミスも起きやすい状態であった。そこで私は、アラート管理ツールを導入してアラートの判断基準の情報を蓄積するようにした。それにより、次回発生した際に対象アラートは自動的に対処要否が判断されるようになった。これにより、月間4000件あったアラートの内、40%が対処不要と割り振られ、その分の対処を減らすことができた。また、アラート管理ツールに対処を行った内容を保存するルールとすることで、俗人化を解消する効果も期待できる。

効果を最大化するために工夫した点として、アラート量と自動処理した割合を可視化することで効果が実感できるようにする点と、入力が滞らないように週１回の定例会議を設けた点である。


## アラート発報条件のしきい値見直し

バッチの処理遅延を検知するために遅延監視を入れていたが、データ増加に伴い年々アラートが増加していた。バッチ遅延により業務に影響がある場合は意味があるが、バッチ時間が少し伸びただけという場合も頻発に発報されるため、無駄なアラートになっていた。アラートが発報されるとインシデント管理システムに登録し問題がない旨を記載するための工数もかかっていた。

原因としては、バッチ開発時点で設定されたしきい値が見直されていなかったことだと考え、アラートを一覧化し遅延監視のしきい値の妥当性の見直しを行うことにした。工夫した点として、打合せにユーザ企業のマネージャーも同席してもらい、当該バッチが遅延することによる業務への影響を議論し、適切なしきい値をその場で決めることができるようにした。また、１回限りの改善で終わらないように４半期に一度、アラートの棚卸としきい値の見直しを行う運用とした。


## サービスデスク

サービスデスクはサービス利用者からインシデント発生の通知を受付、インシデント管理システムに登録する。

- インシデントに対応の優先度（高・中・低）を割り当てる
- 優先度によって、解決目標時間が定められている
  - （高2時間、中４時間、低８時間）


## コミュニケーション

３社同時のビデオミーティング（以下、フォーラム）を開催することを提案して取り決めることにした。フォーラムであれば３社の担当者が同時に顔合わせすることができるので、迅速なコンセンサスを得ることができると考えたためである。

実施後数か月たった後、フォーラムの関連者に対してアンケートを行った結果、フォーラムを行うことで迅速なコンセンサスに寄与しているという解答を得ることができたため、評価としては”良好”であると評価している。


## エラーバジェット
**エラーバジェット**と呼ばれる、定義された**サービスレベル目標（SLO）**を超えてサービスが許容できる年間または月間のダウンタイムやエラーの総量を定量的に示す手法を使用し、**サービスリリースの頻度と安定性のバランス**を取ることを目指した。なぜならば、**リリース後のインシデント急増により、システムの安定稼働を優先しすぎるあまり、ビジネス要求の高い新機能開発のデリバリー速度が著しく低下している**というサービス運用上の問題・課題が解決できると考えたからである。この問題の背景には、特に月初めのアクセス集中時など**ビジネスクリティカルな時間帯**において、インシデント発生時の**顧客満足度スコアが平均30%も低下する**という**サービス品質**の制約があり、過度な品質重視が**開発チームの心理的な負担**にもなっていた。工夫した点は、サービス安定性の確保を目的とした運用作業（パッチ適用、リファクタリング、監視強化など）と、新機能開発のためのリリース作業とを、エラーバジェットの残量に応じて戦略的に切り分ける**ガバナンス体制**を導入することにした。具体的には、この改善を行うことにしたサービスの**SLOとして稼働率99.9%**を設定し、この$0.1\%$分の許容誤差をエラーバジェットとして定義する。そして、月次のエラーバジェットの残量が$50\%$を下回った場合、新たな機能リリースを一時的に停止し、**MTTR（平均復旧時間）を現在の平均60分から45分以下に削減する**ための安定性向上の作業（例：モニタリングアラートの精度向上や自動復旧スクリプトの導入）を優先的に行うことにした。これにより、リスクをコントロールしながらも、安定性を損なわない範囲で開発チームが自信を持って新機能をリリースできる環境を整備し、ビジネス要求への対応速度を向上させることを目指す。 

## 再発防止の記録と見直し（問題管理・KEDBという考え方を明示）
インシデント収束後に根本原因を特定し恒久対策を進める「問題管理」と呼ばれる活動がある。さらに、再発しやすい原因と回避策を蓄積する「KEDB（Known Error DataBase）と呼ばれる、既知の不具合を記録して再発を防ぐためのデータベース」を組み合わせると、再発型インシデントに強い運用になる。夜間のバッチ処理の遅れや外部システムとの通信失敗が月に8件ほど繰り返し発生し、夜間当番の負担と顧客説明コストが増えていたため、これらの考え方を取り入れたモジュールを設けた。
1. サービスが止まる、または極端に遅くなる障害は、翌営業日に必ず「再発防止ログ」に登録し、担当者と期限を決める（問題管理の起点）。
2. 毎週1回、開発と運用が合同で見直し会を開き、効果が大きい対策から計画に組み込む（優先度決定と進捗確認）。
3. 原因、応急の回避策、恒久的な直し方、顧客に説明する文面を一式まとめ、夜間当番の手順書とサービス窓口の回答テンプレートに反映する（KEDBとして活用）。
効果の目標は、同じ種類の障害件数を半期で30%減らすこと、記録を参照した一次対応の平均時間を20分短縮することである。月次レポートで経営と顧客に共有し、改善を続ける。

## オンコールローテーション制度によるインシデント対応の負荷分散
オンコールローテーション制度と呼ばれる、障害対応担当を計画的に輪番で割り当てる手法を使用し、インシデント対応の負荷分散と対応状況の可視化を実現した。なぜならば、特定のメンバーへのインシデント対応の集中により、対応品質のばらつきと平均復旧時間の増加というサービス運用上の問題・課題が解決できると考えたからである。この問題の背景には、24時間365日稼働するサービスにおいて、限られた担当者のみが夜間休日対応を担うことで、担当者の心理的負担が増大し、結果として判断ミスや対応遅延が発生しやすくなるというサービス品質の制約があった。工夫した点は、エスカレーションポリシーとマルチチャネル通知機能を統合したオンコール管理型インシデント管理ツールを選定し、従来のチケット管理システムや単純な監視ツールでは実現できなかった、当番担当者への確実な通知と対応状況のリアルタイム可視化を実現したことである。具体的には、従来は運用リーダー3名のみが監視ツールからのメール通知を受けており、深夜のメール見逃しや休暇中の通知到達失敗が月2～3件発生し、月平均80件のインシデントのうち約60%が同一担当者に集中し、平均復旧時間が約60分かかっていた。そこで対応体制を抜本的に見直し、運用メンバー全8名を対象とした週単位の当番ローテーション制度を確立することで、特定メンバーへの負荷集中を解消した。さらにインシデント発生時には、事前に登録した当番計画から自動的にその週の担当者を判定し、メールだけでなく電話・SMS・モバイルアプリの複数手段で同時に通知することで、深夜や休暇中でも確実に担当者へ到達する仕組みを構築した。加えて一定時間応答がない場合は自動的に次の担当者へエスカレートする機能と、ダッシュボードで誰が対応中かをリアルタイム可視化する機能を活用することで、夜間休日でもチーム全体で支援体制を組めるようにした。結果として、通知到達率が100%となり、通知から初動までの時間が平均15分、平均復旧時間が約35分に短縮され、過去3か月でメンバー全員が対応経験を持つことで属人化も解消され、インシデント対応率が90%を達成した。

## インシデント対応訓練による初動体制の確立
インシデント対応訓練と呼ばれる、重大障害発生時の対応手順と役割分担を定期的に演習する手法を使用し、初動体制の迅速化と組織的な対応力の向上を実現した。なぜならば、サービス停止時の初動対応の遅れと役割の不明確さにより、平均復旧時間の増加と顧客への情報提供遅延というサービス運用上の問題・課題が解決できると考えたからである。この問題の背景には、ビジネスクリティカルなサービスにおいて、重大障害発生時に誰が全体統括し誰が顧客対応するかが不明確で、関係者への連絡や対応手順の確認に時間を要し、結果として顧客への影響が拡大するというサービス品質の制約があった。工夫した点は、緊急事態時の役割として全体統括責任者・対外発信責任者・顧客対応責任者の三役を事前定義し、ワークフロー自動化ツールによる対応手順の自動展開と、四半期ごとの全社参加型訓練を組み合わせたことである。具体的には、当社では過去3年間でサービス全停止レベルの重大インシデントは1回のみ発生したが、その際に役割分担が不明確で初動に45分、顧客への第一報に90分を要し、復旧を最優先するあまり新入社員への教育や改善検討が後回しになった。まれにしか発生しない重大インシデントに対して、実戦だけでは全社員が経験を積めず、また実戦では復旧優先で教育や様々なシナリオの疑似体験が困難であるため、定期訓練による組織的な対応力強化が必要と判断した。そこで緊急事態対応体制を整備し、全体統括責任者には運用部門のマネージャーを、対外発信責任者には広報経験のある企画担当者を、顧客対応責任者にはサポートリーダーをそれぞれ事前にアサインした。さらに緊急事態宣言ボタンを押すと、自動的に対応用チャットチャネルが作成され、三役が自動招集され、対応手順書・顧客向け文面テンプレート・関係者連絡先リストが自動投稿される仕組みを構築した。加えて四半期ごとに、架空の障害シナリオに基づいたロールプレイ訓練を全社員参加で実施し、実戦では経験できない様々なシナリオの疑似体験と、時間をかけた教育・振り返りを行うことで、新入社員も含めた組織全体の対応力を底上げした。結果として、訓練参加率が95%以上を維持し、訓練を通じて初動時間を平均15分、顧客への第一報を平均25分に短縮する目標を設定し、実際の重大インシデント時にも混乱なく対応できる体制が確立された。

## 3-2-1-1-0ルールによるランサムウェア耐性バックアップ
3-2-1-1-0ルールと呼ばれる、従来の3-2-1バックアップルール（3コピー・2種類のメディア・1つはオフサイト）にオフライン保管と変更不可ストレージを追加した手法を使用し、ランサムウェア攻撃に対する復旧能力の確保を実現した。なぜならば、従来の3-2-1ルールでは全バックアップがネットワーク接続されているため、ランサムウェア攻撃時に全バックアップが暗号化されて復旧不能になるというサービス運用上の問題・課題が解決できると考えたからである。この問題の背景には、24時間365日稼働するFX自動取引プラットフォームにおいて、ランサムウェア攻撃により全システムが暗号化された場合、取引停止が長期化し、顧客であるFXブローカーの業務運営に重大な影響を与え、SLA違反と社会的信用の失墜につながるというサービス品質の制約があった。工夫した点は、バックアップの1つをネットワークから物理的に切り離したオフラインストレージとして保管し、クラウド上のバックアップには変更不可設定を適用することで、ランサムウェアが到達できないバックアップを確保したことである。具体的には、従来は本番データベース、同一リージョンのオンラインバックアップ、別リージョンのオンラインバックアップの3コピーを保持していたが、すべてがクラウドネットワーク上に存在し、認証情報漏洩時に全バックアップが削除または暗号化されるリスクがあった。そこで3-2-1-1-0ルールを適用し、3つのコピー（本番、同一リージョンバックアップ、別リージョンバックアップ）、2種類のメディア（データベースストレージ、オブジェクトストレージ）、1つはオフサイト（別リージョン）、1つはオフライン兼イミュータブル（別リージョンのバックアップにオブジェクトロック機能で90日間変更不可設定を適用し、さらに週次で外付けストレージに取得して物理的にネットワークから切断）、0エラー（月次で復元テストを実施し、取引履歴の件数照合と金額照合を行い検証）という構成を確立した。結果として、ランサムウェア攻撃を想定した復旧訓練で、オフラインバックアップから30分以内に取引システムを復旧できることを確認し、RPO 24時間以内、RTO 30分以内の目標達成の見込みが立った。

## CSIRT即応体制と身代金不払い方針による初動対応の確立
CSIRT即応体制と呼ばれる、サイバー攻撃発生時に専門家チームを即座に招集して初動対応を行う手法を使用し、被害拡大の防止と迅速な復旧を実現した。なぜならば、ランサムウェア攻撃などのサイバーインシデント発生時に、初動対応の遅れと対応方針の不明確さにより、被害拡大と復旧時間の長期化というサービス運用上の問題・課題が解決できると考えたからである。この問題の背景には、サイバー攻撃発生時に身代金支払いの判断を現場が迫られ、また情報公開の遅れにより顧客や取引先からの信頼を損なうというサービス品質の制約があった。工夫した点は、DMAT（災害派遣医療チーム）方式を参考にしたCSIRT即応体制を構築し、事前に身代金不払い方針を経営層で決定して全社周知し、さらに情報公開の基準とタイムラインを明文化したことである。具体的には、従来はランサムウェア攻撃を想定した対応体制が未整備で、身代金支払いの判断基準や情報公開のタイミングが不明確であった。そこでCSIRT即応体制を整備し、セキュリティ責任者を統括者、インフラ担当をシステム復旧責任者、法務担当を対外発信責任者として事前にアサインした。さらに経営会議で「いかなる場合も身代金は支払わない」という方針を決定し、全社員に周知するとともに、バックアップからの復旧手順を整備した。加えて情報公開の基準として、インシデント発生から24時間以内に第一報を公式サイトで公開し、影響範囲と復旧見込みを明示することをルール化した。結果として、ランサムウェア攻撃を想定した訓練で、インシデント検知から1時間以内にCSIRT招集、24時間以内に第一報公開という目標を設定し、迅速な初動対応と透明性のある情報公開による信頼維持の体制が確立された。

## リリース承認プロセスの非効率化によるリリースサイクル停滞への対応
リリース承認ワークフロー統合と呼ばれる、承認・検証・通知をCI/CDに集約し条件充足で自動進行できる特徴を持つ手法を使用し、リリース承認プロセスの非効率化によるリリースサイクル停滞への対応を行った。なぜならば、承認者ごとの判断基準ばらつきと手戻りによりリリース遅延と監査証跡の欠落が生じ、**サービス運用上の問題・課題**となっていたからである。工夫した点は、承認情報の確認負荷を下げつつサービス品質のゲートを維持するため、承認に必要な証跡と参照資料を一画面に集約したこと。具体的には、PRに対して必須チェック（テスト結果、リスク評価、バックアウト手順）を満たした場合のみ承認ボタンが表示され、承認時に標準コメントと関連ドキュメントが自動記録されるようにした。
